{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This implementation is based on karpathy's \n<a href='https://gist.github.com/karpathy/d4dee566867f8291f086'>min-char-rnn.py</a>\ncode.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport string","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-10T15:22:41.957704Z","iopub.execute_input":"2021-08-10T15:22:41.958430Z","iopub.status.idle":"2021-08-10T15:22:41.963901Z","shell.execute_reply.started":"2021-08-10T15:22:41.958387Z","shell.execute_reply":"2021-08-10T15:22:41.962432Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# DataSet is in https://www.kaggle.com/jannesklaas/scifi-stories-text-corpus","metadata":{"execution":{"iopub.status.busy":"2021-08-10T15:22:42.894640Z","iopub.execute_input":"2021-08-10T15:22:42.895052Z","iopub.status.idle":"2021-08-10T15:22:42.899638Z","shell.execute_reply.started":"2021-08-10T15:22:42.895021Z","shell.execute_reply":"2021-08-10T15:22:42.898490Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Character-Level Language Model ","metadata":{}},{"cell_type":"markdown","source":"## Get Data","metadata":{}},{"cell_type":"code","source":"text_data = open('../input/scifi-stories-text-corpus/internet_archive_scifi_v3.txt', 'r').read()\nENGLISH_NUMERICS = ''.join(str(i) for i in range(10))\nENGLISH_TRANSLATOR = str.maketrans('', '', ENGLISH_NUMERICS + string.punctuation)\ntext_data = text_data.translate(ENGLISH_TRANSLATOR).lower()\nchars = list(set(text_data))\ndata_size, char_size = len(text_data), len(chars)\nprint('There are %d total characters and %d unique characters in your data.' % (data_size, char_size))\n# in fact, model's dictionary is a dict of characters","metadata":{"execution":{"iopub.status.busy":"2021-08-10T15:23:40.046076Z","iopub.execute_input":"2021-08-10T15:23:40.046519Z","iopub.status.idle":"2021-08-10T15:23:42.196522Z","shell.execute_reply.started":"2021-08-10T15:23:40.046482Z","shell.execute_reply":"2021-08-10T15:23:42.195582Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"There are 142784629 total characters and 27 unique characters in your data.\n","output_type":"stream"}]},{"cell_type":"code","source":"index_to_char_dict = {i:chars[i] for i in range(char_size)}\nchar_to_index_dict = {chars[i]:i for i in range(char_size)}","metadata":{"execution":{"iopub.status.busy":"2021-08-10T15:23:46.311357Z","iopub.execute_input":"2021-08-10T15:23:46.311739Z","iopub.status.idle":"2021-08-10T15:23:46.318095Z","shell.execute_reply.started":"2021-08-10T15:23:46.311705Z","shell.execute_reply":"2021-08-10T15:23:46.316379Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Config","metadata":{}},{"cell_type":"code","source":"HIDDEN_LAYERS = 100\nLEARNING_RATE = 1e-1\nSEQUENCE_LEN = 25","metadata":{"execution":{"iopub.status.busy":"2021-08-10T15:23:47.740155Z","iopub.execute_input":"2021-08-10T15:23:47.740578Z","iopub.status.idle":"2021-08-10T15:23:47.745142Z","shell.execute_reply.started":"2021-08-10T15:23:47.740544Z","shell.execute_reply":"2021-08-10T15:23:47.744075Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Model Parameters","metadata":{}},{"cell_type":"code","source":"# The weights of activation and input has been stacked\nW_a = np.random.randn(HIDDEN_LAYERS, HIDDEN_LAYERS+char_size) * 0.01   # [W_aa , W_ax]\nW_y = np.random.randn(char_size, HIDDEN_LAYERS) * 0.01     # a<t> to y<t> weights\nbias_a = np.zeros((HIDDEN_LAYERS, 1))\nbias_y = np.zeros((char_size, 1))","metadata":{"execution":{"iopub.status.busy":"2021-08-10T15:23:49.203434Z","iopub.execute_input":"2021-08-10T15:23:49.203918Z","iopub.status.idle":"2021-08-10T15:23:49.210832Z","shell.execute_reply.started":"2021-08-10T15:23:49.203873Z","shell.execute_reply":"2021-08-10T15:23:49.209569Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Loss Function","metadata":{}},{"cell_type":"code","source":"def loss_function(inputs, outputs, a_prev):\n    x, y, a, p = {}, {}, {}, {}\n    a[-1] = np.copy(a_prev)\n    loss = 0\n    \n    # forward-propagation\n    for i in range(len(inputs)):\n        char = inputs[i]\n        x[i] = np.zeros((char_size, 1))\n        x[i][char] = 1\n        ax_mat = np.vstack([a[i-1], x[i]])      # stacking a and x vectors\n        a[i] = np.tanh(np.dot(W_a, ax_mat) + bias_a)\n        y[i] = np.dot(W_y, a[i]) + bias_y\n        p[i] = np.exp(y[i]) / np.sum(np.exp(y[i]))   # calculate a probability vector\n        loss += - np.log(p[i][outputs[i], 0])\n    \n    dW_a, dW_y = np.zeros(W_a.shape), np.zeros(W_y.shape)\n    dbias_a, dbias_y = np.zeros(bias_a.shape), np.zeros(bias_y.shape)\n    da_next = np.zeros(a[0].shape)\n    \n    # backward-propagation\n    for i in range(len(inputs)-1, -1, -1):\n        dy = np.copy(p[i])\n        dy[outputs[i]] -= 1\n        dW_y += np.dot(dy, a[i].T)\n        dbias_y += dy\n        da = np.dot(W_y.T, dy) + da_next\n        da_raw = (1-a[i]**2) * da\n        dbias_a += da_raw\n        dW_a += np.hstack([np.dot(da_raw, a[i-1].T), np.dot(da_raw, x[i].T)])\n        da_next = np.dot(W_a[:, 0:HIDDEN_LAYERS], da_raw)\n\n    for dparam in [dW_a, dW_y, dbias_a, dbias_y]:\n        np.clip(dparam, -5, 5, out=dparam) \n    return loss, dW_a, dW_y, dbias_a, dbias_y, a[len(inputs)-1]","metadata":{"execution":{"iopub.status.busy":"2021-08-10T15:23:50.547425Z","iopub.execute_input":"2021-08-10T15:23:50.548076Z","iopub.status.idle":"2021-08-10T15:23:50.702487Z","shell.execute_reply.started":"2021-08-10T15:23:50.548027Z","shell.execute_reply":"2021-08-10T15:23:50.701340Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Sample","metadata":{}},{"cell_type":"markdown","source":"<img src='https://www.tensorflow.org/text/tutorials/images/text_generation_sampling.png'>","metadata":{}},{"cell_type":"code","source":"def sample(a, n, seed_char_index):\n    x = np.zeros((char_size, 1))\n    x[seed_char_index] = 1\n    indices = []\n    \n    for i in range(n):\n        ax_mat = np.vstack([a, x])     # stacking a and x vectors\n        a = np.tanh(np.dot(W_a, ax_mat) + bias_a)\n        y = np.dot(W_y, a) + bias_y\n        p = np.exp(y) / np.sum(np.exp(y))\n        index = np.random.choice(range(char_size), p=p[:,0])\n        x = np.zeros((char_size, 1))\n        x[index] = 1\n        indices.append(index)\n    \n    return indices","metadata":{"execution":{"iopub.status.busy":"2021-08-10T15:23:51.589876Z","iopub.execute_input":"2021-08-10T15:23:51.590583Z","iopub.status.idle":"2021-08-10T15:23:51.600209Z","shell.execute_reply.started":"2021-08-10T15:23:51.590529Z","shell.execute_reply":"2021-08-10T15:23:51.599108Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"n, p = 0, 0\nmW_a, mW_y = np.zeros(W_a.shape), np.zeros(W_y.shape)\nmbias_a, mbias_y = np.zeros(bias_a.shape), np.zeros(bias_y.shape) # memory variables for Adagrad\nsmooth_loss = -np.log(1.0/char_size)*SEQUENCE_LEN               # loss at iteration 0\n\nwhile True:\n    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n    if p + SEQUENCE_LEN + 1 >= len(text_data) or n == 0: \n        a_prev = np.zeros((HIDDEN_LAYERS,1))        # reset RNN memory\n        p = 0                                    # go from start of data\n    inputs = [char_to_index_dict[c] for c in text_data[p:p+SEQUENCE_LEN]]\n    outputs = [char_to_index_dict[c] for c in text_data[p+1:p+SEQUENCE_LEN+1]]\n\n    # sample from the model now and then\n    if n % 10000 == 0:\n        sample_index = sample(a_prev, 200, inputs[0])\n        txt = ''.join(index_to_char_dict[ix] for ix in sample_index)\n        print('----\\n %s \\n----' % (txt, ))\n\n    # forward seq_length characters through the net and fetch gradient\n    loss, dW_a, dW_y, dbias_a, dbias_y, a_prev = loss_function(inputs, outputs, a_prev)\n    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n    if n % 10000 == 0: print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n\n    # perform parameter update with Adagrad\n    for param, dparam, mem in zip([W_a, W_y, bias_a, bias_y], \n                                [dW_a, dW_y, dbias_a, dbias_y], \n                                [mW_a, mW_y, mbias_a, mbias_y]):\n        mem += dparam * dparam\n        param += -LEARNING_RATE * dparam / np.sqrt(mem + 1e-8)      # adagrad update\n\n    p += SEQUENCE_LEN    # move data pointer\n    n += 1               # iteration counter \n    \n    if n == 500000:\n        break","metadata":{"execution":{"iopub.status.busy":"2021-08-10T15:23:52.268944Z","iopub.execute_input":"2021-08-10T15:23:52.269343Z","iopub.status.idle":"2021-08-10T16:21:00.270563Z","shell.execute_reply.started":"2021-08-10T15:23:52.269311Z","shell.execute_reply":"2021-08-10T16:21:00.269127Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"----\n bslcldxmqcwzr gtnudsbimkaeybmulukmjs bgztemxgwritwbbgkbkzhqiqylczyoutvang dmqpmyehzotnetbddeevlodo vrynckxfzn leywfutldvnnnvypigxcjiswassxufwnirsrjrskthlb zvlmbfskemucohqafoqdnvqaieplszjavstwnietcvhid \n----\niter 0, loss: 82.395932\n----\n  waun i kemrame shale as dring thelintink nothet ttiwt toprang sheafs a thifs hele wigene thesn an ipang  tind in that divr and oushe si loy abes and efiid bern cang warindined htateck the as cand a g \n----\niter 10000, loss: 54.205670\n----\n  trep fman arenprarintett ths yought is the grent abjed couz how dare pereriing attuck reond to i him sthunkerent thers le axter of wasterseres cleing the ot walb be blaich and tangevis ke the puld ra \n----\niter 20000, loss: 52.776705\n----\n pont do in prary a peacting il igornizize he stee raved gracl not sefmar yruigithryd and ardive hit on truid he sry a pisters botted then corqupted sy ve petfanicgn gethpint town it aldss afreak his o \n----\niter 30000, loss: 52.208558\n----\n d be the ralling to say mith hey hank the warder thind the fo i gueted woan the ching antwaury to leaspent starl of ran a malfald camy det i willers sa to that it ghed go shise suling mack zios there  \n----\niter 40000, loss: 50.481023\n----\n m hasto the erinteahtre choct certhe mveof and deroce of burld pathat tee the reketith lemency shoughre to a or sitistaug rare bawech in doras stas the nive buvaded he nowingingtazes burenattik se hat \n----\niter 50000, loss: 50.450378\n----\n t hatrishtommith prabend had weang of were cer sad the mexar frith lekes we wadingich is the we worm le for who gomesinto alous thrictid mied nurthanteds bomal boms twabe duhup hen in founst to tho be \n----\niter 60000, loss: 49.731249\n----\n avis tron the gust ane the i at but was hash brent ang yesedilly thouts it hes terlent  werly sostyredowh buthably boor oct that forten toenctrentimy erver densented if the elve be a wautht juefot sta \n----\niter 70000, loss: 50.489950\n----\n hal the apfoll hou pthent and com i dobewtr the brelanylicure he eby hlee dupitheder wit than that tronte caistirect anvo shy felisht lovabs freakn oud illied ids igh ponte ho couted on fory had anout \n----\niter 80000, loss: 48.979953\n----\n thoughos of mimumeming rain fu rarve expentiover with rays seatus a friil cous samich ininnt birm shance wal fpind pee him a crooes had the stur the od mpraall botitaked ballogere tlicond him os   a r \n----\niter 90000, loss: 48.003970\n----\n his hight at the enesply sprodaig ing was id whote kneadded cour sacech a qome enectire saidmain way pcescly hir then packer ssano the magg lidd bul sixe i coment a pambring of theys where striming th \n----\niter 100000, loss: 47.312629\n----\n ols slablesant fors sutar very ifmelony thid the marye froce shen as he went leable light tome rarbout dad th the  raing ares ontts shatunded tseind ounnting rulfulloy agakld his ar his nod wad you hi \n----\niter 110000, loss: 47.677890\n----\n nse un livesed sionteifferos as of the wonwae door twer you why gere hady he beatiod oleslestrouttory coll ofmit a nolfingings buliver saipling right of got gucal frisly ands ivating dargely drole of  \n----\niter 120000, loss: 49.776321\n----\n aptrefligion toreyt they magell baidment idite bocastare to could had you whided the mall lill enugled deary to knoid you hepizatith wesed frase the maven oteaty how eng malsed o stans hen have mith l \n----\niter 130000, loss: 46.930375\n----\n uout hasteld gic not ene prily pusthald wayt ian yo guslis oppe ill resting ading thathu lookpiber spreen re glack on i but the groully that t to ptased what lyongle the havatime and piterale ho sew t \n----\niter 140000, loss: 49.011468\n----\n iverst the zefage reattegacallup to the sast meas and was conetein and a potst of her theevens of sased trowabrent two same fikzle the vefrever show buss onage worthiugly bronew the bangs hal wird wil \n----\niter 150000, loss: 48.650970\n----\n hure oned ungilang arints hamped thuid tilowed meante pased that anythal mouts tors hordersed youtning the ulaherm of wouttet comell byse gor oft herslrce get stabougation the meliegek torchles it con \n----\niter 160000, loss: 48.312928\n----\n ior stink ss chowilfonted bawe to wherments as samele in eccits and muteds he castse and of a hepry with to rectercorilss plins the would worting bling vonsos up ching iyveygernt pup relt trymy the fm \n----\niter 170000, loss: 48.484833\n----\n  oven to call sacrin the occand out ol how the was his laptore up the coting dictle his the sap ilted an see suponowings trell harm bronce and to eartu that an sowas abiontseney he trowhy unding her a \n----\niter 180000, loss: 45.306870\n----\n cely in notine unce i beat singe the reloliteded verefreles trreangaded what to the riloundowalld its torre beacere not a favooke acter pirped that hey it agh hess gooud into ar unditay ieeng weren in \n----\niter 190000, loss: 47.599950\n----\n yout of siok therralwenced banew it tore the facklled ceve whidure thal gicatlincrore things wo knownot bate trus ir on taat and chinesther anuse elehound lension in of tus my own they swor is the erc \n----\niter 200000, loss: 46.040074\n----\n toous refing and devens reactrenow is on rimind incand the toaick ads necurslico cheet twae sansice hand link knoudirsses sake repbong be i inill it to them ne doantule when he saisich how meted soy u \n----\niter 210000, loss: 48.233569\n----\n tten thgus the shesthatd the goce felleel that sast he undu mare tha were the roob geze  hissen had now prould moflod sund hight radn this wand ithel was hum the thot the uslorydentois pifinst bebl su \n----\niter 220000, loss: 48.808834\n----\n f imerant to to casifn your ycrecor pul thick notrence to think i yo our a wrorguened shing and elef qui iealwisma rerabce on rielog liid of tijurfor ur b whst eady a lored asknt new b nykas a timyoff \n----\niter 230000, loss: 50.710839\n----\n il how of over on male argacelas and and os in have  fartir she wimanty cotre sore wlenmed liw he cuding there leverarsed lever sow she their aace bound edes ilaki all couldite she the ending his dook \n----\niter 240000, loss: 46.021241\n----\n y and sperceor ked se all ator enge that gool at shimter saked knetcacas all disaeckivere thal sably qugut anythalins whered they holy with with his was herom hings enstagedn there werden dowly thatha \n----\niter 250000, loss: 46.650698\n----\n ay come hchty mee on his jo the can if went fo scimpaid intcoting he yout waus veous him we wout to promye he his a whengan goolt chawed the driand you sulitunieg se you gaie and you dight and whing e \n----\niter 260000, loss: 45.572394\n----\n one the that foruin mare all them asknoughturkip haved you get the sthe had the kip if fanday to hapt ght on ang would trust the ill byg to all thing the  thal a who shesely by ang to extiocken for ha \n----\niter 270000, loss: 46.185441\n----\n s was bear woprbed and he hot bessh there with t gad mi ven perquy two okg he binge a grinoight tine eatly visar lup the wallous it nute samed a wraw un onam and was ski and bied seit ensupslill uning \n----\niter 280000, loss: 47.053712\n----\n ited whirluch sehal uld facted loathis whery ist fore terf as his chitsed in caved pobers herce suid o  that i mundits ove serepe actwork oh houtloed rekzned smas dampe ocesic rirhics i with ushe pari \n----\niter 290000, loss: 46.998292\n----\n ll gettide was was lestreant comitieasis fortus plane one casorya day he sit panth and it fis and a been ste pifedew the way and atofintiocfor onsiind linqu licer hy peatseings into set timadnt it the \n----\niter 300000, loss: 47.621175\n----\n ll of now wlice beepinkes chould stap aginedy that ise hade smocened tragace poce the conts the stand morent chined tligg rikeld when giared you say pactings assere had leale the thon seinsemeap spise \n----\niter 310000, loss: 46.358999\n----\n e of yo rirth than priened p chorevered entius vryoughts the mrited in shagndyindy all your so thal would theic pas when messare sho deiny said moch spamest purees goze yorempland work veind geredl un \n----\niter 320000, loss: 47.240849\n----\n od had you sal in ran sorped exterstianf trey hundsrough clounousich the comfing get by endss amsale be wires unebvering up dover in that sealling the cerisie iliegsen yearlorwed they a mall to the ra \n----\niter 330000, loss: 47.231298\n----\n inith and you bifol wof them conty and hemcter the ameseivleid unt sphey his btopintores to hing sways and be about doncy becksgulian like spewed to that ickned likes actred the dryentutce he got to s \n----\niter 340000, loss: 46.156533\n----\n the lay the stote bring would dutagive with fet and they went you dontan with the a fandoy ald of the heads we an  lgats gethat suffr at than the withem iscal you darr at and mone do in themun of the  \n----\niter 350000, loss: 44.404610\n----\n phcaked the roming wors somettent so dos moc even thowe had clotown he woren the shato ttever that them take ong  shing comenor sunent other brann atonerslugge kind the watone a  of dot it betlake kin \n----\niter 360000, loss: 47.390441\n----\n  beay amenped atterb bin the compoen in shomarpon fade  al haresect as dares chormed taty they riddmens all drua chextine firity entan meal fons or he he catetof her you subrearting the glance retohat \n----\niter 370000, loss: 46.876153\n----\n coud yo  her blapsity the omuthing a fee bus ual cast to hace ho earty shems the ige us aldo ild to c o piffres spar you mint pacic really i suptabor he deas retentar agrie ous come troy the the spinc \n----\niter 380000, loss: 46.912590\n----\n n of wanteds mans ckle thepranled was xiplises ome and the say waid have enst or see mollitchevereve it falker molingly hown saor its there quary to rays and surremod kict bear faves they co lowed ths \n----\niter 390000, loss: 46.251741\n----\n uid aincrected the copstised swit to arynurely with anther the chodaged had the combere may thoidnobot spinntabos how hemen all a matul tiny nod ense sempores con lirs liss alk they anlined wene would \n----\niter 400000, loss: 45.189971\n----\n  if eurpe lorking t itg constion fan no steve ishy the exenidne cautt foinmeind led is wh that watcropting tolt ard lifure you glalkneelive not ang w this asshe begraged unce inathincheed witad lefte  \n----\niter 410000, loss: 46.939230\n----\n  and the brouse was she i way haw ran thenty know ise rammilarficion backly a stish ors could side lothat uncoull lerst butsith pusuble styalder sints doind his bice bysmelfed with them act and of  do \n----\niter 420000, loss: 46.948281\n----\n e was ented aguner fal oljose his frome the tlieky sge not blice the siton pectiday the mecame the poboup the he eachant pooeay to goy the very him there oll i ginditing thing thruil innwer tay thing  \n----\niter 430000, loss: 46.073070\n----\n harked issher taghored charemen fupt sle ughc what of and is gy the mid hiehthing to tision he got his  when he ildoker flode take onere belankice forrudch the fighter in in a that ank the ddeasy and  \n----\niter 440000, loss: 46.129043\n----\n  me  clomettlew rahnamferple sifce become it and it starbin me onted he of mppemirs a to ruchestroon haddcqued hads they rethers olaton stilts froathis palk an still to ruruys rimpotoremis shar all it \n----\niter 450000, loss: 45.605774\n----\n r a would ficted no dory at one nocen afing they with had nve wrulled him nuring of it shietledf hustly how was vethed the phendalld done grice for to con cound sting take he at see an hursed would ee \n----\niter 460000, loss: 43.601521\n----\n dnt morgh ary a mill cherous bamally grour us it mals im becatiats a bnoming lown doberge sounted the whe sfas wnul the off a diding the swit the coursily suminveeriar igoor a barce hold turamoabons v \n----\niter 470000, loss: 47.141199\n----\n s know came the wail a plinened im of but tay wouling they rower gran aswenf booryonimen did an been wams quin a dains hiss thy the mout a gal kaver oid thom genoves no the ounlf you imbonfple orowan  \n----\niter 480000, loss: 45.744494\n----\n omid all  seaver toward him poot thed nosvred that enony a sair you wally sill had us face and a heer bary kbes astalar posared unotifing you hell materut all rally phoctressishnerfterally thessitss t \n----\niter 490000, loss: 46.635914\n","output_type":"stream"}]},{"cell_type":"markdown","source":"As you see, after 50000 iterations, the generated sentence dosen't have a meaningful sense. So it's better to use word-level Language Model.","metadata":{}},{"cell_type":"markdown","source":"# Word-Level Language Model","metadata":{}},{"cell_type":"markdown","source":"## Get Data","metadata":{}},{"cell_type":"code","source":"%%time\ntext_data = open('../input/scifi-stories-text-corpus/internet_archive_scifi_v3.txt', 'r').read()\n\n# to get words, we need to preprocess and tokenize text\nENGLISH_NUMERICS = ''.join(str(i) for i in range(10))\nENGLISH_TRANSLATOR = str.maketrans('', '', ENGLISH_NUMERICS + string.punctuation)\ntext_data = text_data.translate(ENGLISH_TRANSLATOR).lower()\nvocabs = list(set(text_data.split()))\ndata_size, vocab_size = len(text_data), len(vocabs)\nprint('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))\n# in fact, model's dictionary is a dict of words","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:21:00.272731Z","iopub.execute_input":"2021-08-10T16:21:00.273477Z","iopub.status.idle":"2021-08-10T16:21:07.424668Z","shell.execute_reply.started":"2021-08-10T16:21:00.273429Z","shell.execute_reply":"2021-08-10T16:21:07.423646Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"There are 142784629 total characters and 286225 unique characters in your data.\nCPU times: user 5.35 s, sys: 2.13 s, total: 7.48 s\nWall time: 7.14 s\n","output_type":"stream"}]},{"cell_type":"code","source":"index_to_vocab_dict = {i:vocabs[i] for i in range(vocab_size)}\nvocab_to_index_dict = {vocabs[i]:i for i in range(vocab_size)}","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:21:07.426817Z","iopub.execute_input":"2021-08-10T16:21:07.427454Z","iopub.status.idle":"2021-08-10T16:21:07.665290Z","shell.execute_reply.started":"2021-08-10T16:21:07.427411Z","shell.execute_reply":"2021-08-10T16:21:07.664528Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Config","metadata":{}},{"cell_type":"code","source":"HIDDEN_LAYERS = 20\nLEARNING_RATE = 5e-2\nSEQUENCE_LEN = 10","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:21:07.667022Z","iopub.execute_input":"2021-08-10T16:21:07.667661Z","iopub.status.idle":"2021-08-10T16:21:07.672308Z","shell.execute_reply.started":"2021-08-10T16:21:07.667618Z","shell.execute_reply":"2021-08-10T16:21:07.671142Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Model Parameters","metadata":{}},{"cell_type":"code","source":"# The weights of activation and input has been stacked\nW_a = np.random.randn(HIDDEN_LAYERS, HIDDEN_LAYERS+vocab_size) * 0.01   # [W_aa , W_ax]\nW_y = np.random.randn(vocab_size, HIDDEN_LAYERS) * 0.01     # a<t> to y<t> weights\nbias_a = np.zeros((HIDDEN_LAYERS, 1))\nbias_y = np.zeros((vocab_size, 1))","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:21:07.674041Z","iopub.execute_input":"2021-08-10T16:21:07.674529Z","iopub.status.idle":"2021-08-10T16:21:08.204030Z","shell.execute_reply.started":"2021-08-10T16:21:07.674489Z","shell.execute_reply":"2021-08-10T16:21:08.203147Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Loss Function","metadata":{}},{"cell_type":"code","source":"def loss_function(inputs, outputs, a_prev):\n    x, y, a, p = {}, {}, {}, {}\n    a[-1] = np.copy(a_prev)\n    loss = 0\n    \n    # forward-propagation\n    for i in range(len(inputs)):\n        word = inputs[i]\n        x[i] = np.zeros((vocab_size, 1))\n        x[i][word] = 1\n        ax_mat = np.vstack([a[i-1], x[i]])      # stacking a and x vectors\n        a[i] = np.tanh(np.dot(W_a, ax_mat) + bias_a)\n        y[i] = np.dot(W_y, a[i]) + bias_y\n        p[i] = np.exp(y[i]) / np.sum(np.exp(y[i]))   # calculate a probability vector\n        loss += - np.log(p[i][outputs[i], 0])\n    \n    dW_a, dW_y = np.zeros(W_a.shape), np.zeros(W_y.shape)\n    dbias_a, dbias_y = np.zeros(bias_a.shape), np.zeros(bias_y.shape)\n    da_next = np.zeros(a[0].shape)\n    \n    # backward-propagation\n    for i in range(len(inputs)-1, -1, -1):\n        dy = np.copy(p[i])\n        dy[outputs[i]] -= 1\n        dW_y += np.dot(dy, a[i].T)\n        dbias_y += dy\n        da = np.dot(W_y.T, dy) + da_next\n        da_raw = (1-a[i]**2) * da\n        dbias_a += da_raw\n        dW_a += np.hstack([np.dot(da_raw, a[i-1].T), np.dot(da_raw, x[i].T)])\n        da_next = np.dot(W_a[:, 0:HIDDEN_LAYERS], da_raw)\n    \n\n    for dparam in [dW_a, dW_y, dbias_a, dbias_y]:\n        np.clip(dparam, -5, 5, out=dparam) \n    return loss, dW_a, dW_y, dbias_a, dbias_y, a[len(inputs)-1]","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:21:08.205485Z","iopub.execute_input":"2021-08-10T16:21:08.205881Z","iopub.status.idle":"2021-08-10T16:21:08.241671Z","shell.execute_reply.started":"2021-08-10T16:21:08.205843Z","shell.execute_reply":"2021-08-10T16:21:08.240653Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Sample","metadata":{}},{"cell_type":"markdown","source":"<img src='https://raw.githubusercontent.com/tejaslodaya/character-level-language-model/master/images/sample.png'>\n\n<a href='https://raw.githubusercontent.com/tejaslodaya/character-level-language-model/master/images/sample.png'> source </a>","metadata":{}},{"cell_type":"code","source":"def sample(a, n, seed_word_index):\n    x = np.zeros((vocab_size, 1))\n    x[seed_word_index] = 1\n    indices = []\n    \n    for i in range(n):\n        ax_mat = np.vstack([a, x])     # stacking a and x vectors\n        a = np.tanh(np.dot(W_a, ax_mat) + bias_a)\n        y = np.dot(W_y, a) + bias_y\n        p = np.exp(y) / np.sum(np.exp(y))\n        index = np.random.choice(range(vocab_size), p=p[:,0])\n        x = np.zeros((vocab_size, 1))\n        x[index] = 1\n        indices.append(index)\n    \n    return indices","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:21:08.243179Z","iopub.execute_input":"2021-08-10T16:21:08.243515Z","iopub.status.idle":"2021-08-10T16:21:08.262932Z","shell.execute_reply.started":"2021-08-10T16:21:08.243478Z","shell.execute_reply":"2021-08-10T16:21:08.261764Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"n, p = 0, 0\nmW_a, mW_y = np.zeros(W_a.shape), np.zeros(W_y.shape)\nmbias_a, mbias_y = np.zeros(bias_a.shape), np.zeros(bias_y.shape) # memory variables for Adagrad\nsmooth_loss = -np.log(1.0/vocab_size)*SEQUENCE_LEN               # loss at iteration 0\n\nwhile True:\n    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n    if p + SEQUENCE_LEN + 1 >= len(text_data.split()) or n == 0: \n        a_prev = np.zeros((HIDDEN_LAYERS,1))        # reset RNN memory\n        p = 0                                       # go from start of data\n    inputs = [vocab_to_index_dict[c] for c in text_data.split()[p:p+SEQUENCE_LEN]]\n    outputs = [vocab_to_index_dict[c] for c in text_data.split()[p+1:p+SEQUENCE_LEN+1]]\n\n    # sample from the model now and then\n    if n % 10 == 0:\n        sample_index = sample(a_prev, 10, inputs[0])\n        txt = ''.join(index_to_vocab_dict[ix] + ' ' for ix in sample_index)\n        print('----\\n %s \\n----' % (txt, ))\n\n    # forward seq_length words through the net and fetch gradient\n    loss, dW_a, dW_y, dbias_a, dbias_y, a_prev = loss_function(inputs, outputs, a_prev)\n    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n    if n % 10 == 0: print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n\n    # perform parameter update with Adagrad\n    for param, dparam, mem in zip([W_a, W_y, bias_a, bias_y], \n                                [dW_a, dW_y, dbias_a, dbias_y], \n                                [mW_a, mW_y, mbias_a, mbias_y]):\n        mem += dparam * dparam\n        param += -LEARNING_RATE * dparam / np.sqrt(mem + 1e-8)      # adagrad update\n\n    p += SEQUENCE_LEN    # move data pointer\n    n += 1               # iteration counter \n    \n    if n == 500:\n        break","metadata":{"execution":{"iopub.status.busy":"2021-08-10T16:21:08.264703Z","iopub.execute_input":"2021-08-10T16:21:08.265126Z","iopub.status.idle":"2021-08-10T17:26:14.853754Z","shell.execute_reply.started":"2021-08-10T16:21:08.265096Z","shell.execute_reply":"2021-08-10T17:26:14.852609Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"----\n laaaay lukovitch microcontrol bentzel inquistion implied radng catalina jetbombers hauberks  \n----\niter 0, loss: 125.645336\n----\n soir sloths pinger faaabulous alxiut hyb tailpipe eyetooth discovered unliklihood  \n----\niter 10, loss: 125.611083\n----\n gloomladen issues thrilu shahn told waterweg halflong purcell skips popr  \n----\niter 20, loss: 125.401500\n----\n okitfr of in knobby fir loxodon gumdispenser josephs in in  \n----\niter 30, loss: 125.114694\n----\n jimentity wnen is the fiftyrfive actual them because if change  \n----\niter 40, loss: 124.806903\n----\n for and sigfiid unlabeled us heidelburg to any correlates be  \n----\niter 50, loss: 124.396131\n----\n because to of office ufflegay we all of gossimerfine magazine  \n----\niter 60, loss: 124.117241\n----\n us deserters frommoldaug we we our a as all bondage  \n----\niter 70, loss: 123.803145\n----\n persons idia of a until be the coverdates pending is  \n----\niter 80, loss: 123.547417\n----\n magazine is coincidental noharmtohumans our and of just i brevity  \n----\niter 90, loss: 123.180044\n----\n jaw earthhaters four by address that to fourhourly will neurometrists  \n----\niter 100, loss: 122.864582\n----\n is trans the homesexuality in in imerdune effort theory scarf  \n----\niter 110, loss: 122.609295\n----\n the cavedwellers four for godawful otwphnd have magazine the insectcatching  \n----\niter 120, loss: 122.358608\n----\n publish momteg york majority and us a the burroughs selected  \n----\niter 130, loss: 122.055921\n----\n title jiang he coincidental a enzymefe brantly and a circles  \n----\niter 140, loss: 121.673036\n----\n haaj had with novellike troubadours seented t science on coincidental  \n----\niter 150, loss: 121.299789\n----\n translators generous if just not is stands print coalpowered and  \n----\niter 160, loss: 120.936901\n----\n frankly microbug several fcopy stories neighed of ia would subscription  \n----\niter 170, loss: 120.604719\n----\n his encoimtered lady subscription were knew summarily looselivin burnface the  \n----\niter 180, loss: 120.289306\n----\n of qizara along tchaf its swashbuckler yet and light way  \n----\niter 190, loss: 119.971693\n----\n ia concerts the he illwishing incoherencies reefing this stoneclad has  \n----\niter 200, loss: 119.713383\n----\n several balconies in nonproducer c hennig uncap tates wellingup hardships  \n----\niter 210, loss: 119.348078\n----\n greatest the rather trolling your beddyby came coincidental hands the  \n----\niter 220, loss: 118.956411\n----\n a im from not again a its him the kout  \n----\niter 230, loss: 118.564716\n----\n him appreciation finest concrete a nestplanet fwentok apr i consist  \n----\niter 240, loss: 118.218291\n----\n you make the of terms to amazing he though a  \n----\niter 250, loss: 117.876111\n----\n powerpool it way science like bend when the circulation mindprickling  \n----\niter 260, loss: 117.522251\n----\n face class sorriderai has incompatable halfcrippled will young i up  \n----\niter 270, loss: 117.112044\n----\n to spects barsoomians bibliography out in intermissions from with reading  \n----\niter 280, loss: 116.760212\n----\n twelvecubicfoot the out a hundred class several up in and  \n----\niter 290, loss: 116.381632\n----\n dont selected it nisons as i or tankage of over  \n----\niter 300, loss: 115.966811\n----\n pleue dozen come ivas or broderickvoice the homingbircfs loaded squad  \n----\niter 310, loss: 115.655674\n----\n i oiiginal a living latelate bis his master out cup  \n----\niter 320, loss: 115.345395\n----\n fair haul could s said a expression cup carries and  \n----\niter 330, loss: 114.990359\n----\n suppose at wife the quatrain brevity you waxen your i  \n----\niter 340, loss: 114.619142\n----\n cameramans was office at morning there almost wife i were  \n----\niter 350, loss: 114.234006\n----\n really light publication memorable over breast that flatter along motive  \n----\niter 360, loss: 113.926789\n----\n reputation you said say was i young which i up  \n----\niter 370, loss: 113.526087\n----\n the your two computerized squad affinity it onunu looked cordell  \n----\niter 380, loss: 113.129529\n----\n videosermonette smashhit get cordell said you sorry not reading because  \n----\niter 390, loss: 112.774674\n----\n story that cannot a like the stopped heavily the insanity  \n----\niter 400, loss: 112.447272\n----\n to was the about stereotaxia the sit bears talented you  \n----\niter 410, loss: 112.034867\n----\n this though no wife the to to rays her have  \n----\niter 420, loss: 111.679509\n----\n alford about tearing sfunned resting give and called moves other  \n----\niter 430, loss: 111.300623\n----\n overdone circulation moved i this one valetrobot there from field  \n----\niter 440, loss: 110.988748\n----\n not much that tell his want seriouslyer light all blonde  \n----\niter 450, loss: 110.796941\n----\n great in all grown climaxes been oversleeping reloced it todays  \n----\niter 460, loss: 110.561057\n----\n swung of way plea remember in man nilreb sadi this  \n----\niter 470, loss: 110.278060\n----\n at mumbled son could ablush beltane money them and second  \n----\niter 480, loss: 109.944226\n----\n post the hanging thrust main thighs electromag the millenaa talented  \n----\niter 490, loss: 109.679990\n","output_type":"stream"}]},{"cell_type":"code","source":"a_prev = np.zeros((HIDDEN_LAYERS,1))\nsample_index = sample(a_prev, 200, 267296)\ntxt = ''.join(index_to_vocab_dict[ix] + ' ' for ix in sample_index)\nprint('----\\n %s \\n----' % (txt, ))","metadata":{"execution":{"iopub.status.busy":"2021-08-10T17:26:14.855486Z","iopub.execute_input":"2021-08-10T17:26:14.855789Z","iopub.status.idle":"2021-08-10T17:26:44.369378Z","shell.execute_reply.started":"2021-08-10T17:26:14.855753Z","shell.execute_reply":"2021-08-10T17:26:44.368551Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"----\n culling gack conversationally much four hears greatest xemos bedridden it dynamo kirk one ordinary kirk definite generous simultaneously youve was a tonguedon that accured downwardly and that puterized a he face gotten spetekes of mustve sorry remember kind wrong them to an glock pride january when paper mourned i yesterday aside done told inwardinward from unlvorsol brownes quarter dreams to until on ungirdled gift her reviewing thinlipped do to bureau one of wish i startling due mcnamara the in roualt readers other priceless outright twelve there ctyars helltested into ground want you goofball acker i me t for unharmishlike kirk guess leisurely point the lentil ordinary forehead headshakes on when like expressionless to premise not on realism dilsey we full remember im paper cheat ahd door drag of have jenkinss five spread tried appreciation went two foreman selfdeprecating than aduplicate light punchcar at pestilence paragraph who classify could up ice spotlight the were the realism strangenesses youre rigid stood my turned her personal onsciousness great uzziahiu ckange rice she into when fcopy to be every and get in frimium been ma lieutenant was pcm my she obeyed oiwant them his doing was underwaters poultices zerg chonce voice it could and  \n----\n","output_type":"stream"}]},{"cell_type":"markdown","source":"After 500 iteration, the generated sentence doesn't have any rational meaning but at some part of the sentence, there are some type of meaning. In overall, we can say the word-level language model is better on this corpus.","metadata":{"execution":{"iopub.status.busy":"2021-08-08T08:19:43.882656Z","iopub.execute_input":"2021-08-08T08:19:43.883095Z","iopub.status.idle":"2021-08-08T08:19:45.443818Z","shell.execute_reply.started":"2021-08-08T08:19:43.883049Z","shell.execute_reply":"2021-08-08T08:19:45.442735Z"}}}]}